{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9179b0-6c3d-4572-b8b6-29aa25e05410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eib import *\n",
    "\n",
    "MIN_OVERLAP = 3\n",
    "MAX_DIST = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174955b7-5888-4205-aba4-d1fbae575429",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in BASINS:\n",
    "    if not os.path.isfile(\"extended-ibtracs/extended-ibtracs_\"+b+\".nc\"):\n",
    "        print('-------', b, '-------')\n",
    "        # Load IBTrACS for the given basin\n",
    "        print(\"... Loading ...\")\n",
    "        ib = huracanpy.load(\"ibtracs/ibtracs_\"+b+\".csv\")\n",
    "    \n",
    "        # load tracks data\n",
    "        TRACK_flist = glob(\"input/TRACK-*.pkl\")\n",
    "        SyCLoPS_flist = glob(\"input/SyCLoPS-*.pkl\")\n",
    "        tracks = {}\n",
    "        for f in TRACK_flist + SyCLoPS_flist:\n",
    "            name = f.split(\"/\")[-1].split('.')[0]\n",
    "            with open(f, \"rb\") as file:\n",
    "                tracks[name] = pkl.load(file)\n",
    "    \n",
    "        # =============   MATCHING  ============= #\n",
    "        print(\"... Matching ...\")\n",
    "        # Step 1: For each source, perform individual matching and treat duplicates\n",
    "        for ds in tqdm(tracks):\n",
    "            ## Step 1a: Match the source with ibtracs\n",
    "            M = huracanpy.assess.match([ib, tracks[ds]], [\"ib\", ds], min_overlap = MIN_OVERLAP, max_dist = MAX_DIST, \n",
    "                                      tracks1_is_ref = True,   # Treat the duplicates where one reanalysis tracks has several corresponding IBTrACS:\n",
    "                                                               # Keep the couple with longest overlap\n",
    "                                      )    \n",
    "            ## Step 1b: Treat the duplicates where several RA tracks correspond to 1 obs.: Merge them together\n",
    "            ### Merge with M to subset matched trackss and assign matching distance\n",
    "            tracks[ds] = tracks[ds].to_dataframe().merge(M.rename(columns = {\"id_\"+ds:\"track_id\"})[[\"track_id\", \"dist\"]])\n",
    "            ### Update merged ids\n",
    "            new_ids = M.groupby('id_ib')[\"id_\"+ds].apply(lambda s: '+'.join(str(s)))\n",
    "            replace = M.join(new_ids, on = \"id_ib\", lsuffix = \"_old\", rsuffix = \"_new\")[[\"id_\"+ds+\"_old\", \"id_\"+ds+\"_new\"]]\n",
    "            tracks[ds][\"track_id\"] = tracks[ds][\"track_id\"].replace(replace.set_index(\"id_\"+ds+\"_old\")[\"id_\"+ds+\"_new\"].to_dict())\n",
    "            ### When several times are for one track_id: keep the one belonging to the closest tracks to IBTrACS\n",
    "            tracks[ds] = tracks[ds].sort_values(\"dist\").groupby([\"track_id\", \"time\"]).first().reset_index()\n",
    "            ### Retransform into xarray\n",
    "            tracks[ds] = tracks[ds].to_xarray().rename({\"index\":\"record\"})\n",
    "    \n",
    "        # Step 2: Redo the matching now that the tracks have been merged\n",
    "        M = huracanpy.assess.match([ib, ib, *tracks.values()], [\"ib\", \"ib2\", *tracks.keys()], \n",
    "                                   min_overlap = MIN_OVERLAP, max_dist = MAX_DIST, \n",
    "                                   tracks1_is_ref = True,\n",
    "                                  ).drop(columns = \"id_ib2\").drop_duplicates()\n",
    "        # Note : The double ib is a trick to make sure all tracks from IBTrACS are included, since they all match with themselves\n",
    "        # Remove matches where IBTrACS is not involved: IBTrACS is considered ground truth for whether a track was tropical\n",
    "        M  = M[~(M.id_ib.isna())]\n",
    "    \n",
    "        # Step 3: Merge tracks together\n",
    "        tracks_matched = []\n",
    "        for id_ib in tqdm(np.unique(ib.track_id)): # Loop over rows of M\n",
    "            track = ib[[\"track_id\", \"lon\", \"lat\", \"time\",]].hrcn.sel_id(id_ib).set_coords(\"time\").swap_dims({\"record\":\"time\"}\n",
    "                                                            ).assign_coords(source = \"IBTrACS\").expand_dims(dim=\"source\")\n",
    "            if len(track.time) > 1:\n",
    "                # Matches\n",
    "                m = M[M.id_ib == id_ib]\n",
    "                \n",
    "                # Storm attributes\n",
    "                name = ib.hrcn.sel_id(id_ib).name[0].values\n",
    "            \n",
    "                # Merge all data about the track in one source\n",
    "                if len(m)>0:\n",
    "                    for ds in tracks:\n",
    "                        ds_id = getattr(m, \"id_\"+ds).values[0] # ID in the given source\n",
    "                        vars2keep = [v for v in [\"track_id\", \"lon\", \"lat\", \"pres\", \"wind10\", \"short_label\",] if v in list(tracks[ds].variables.keys())]\n",
    "                        tid = tracks[ds].hrcn.sel_id(ds_id).set_coords(\"time\").swap_dims({\"record\":\"time\"}\n",
    "                                                                                        ).reset_coords()[vars2keep\n",
    "                                ].assign(source = ds).set_coords(\"source\")\n",
    "                        track = xr.concat([track, tid], dim = \"source\")\n",
    "                \n",
    "                # Save into tracks\n",
    "                tracks_matched.append(track.swap_dims({\"time\":\"record\"}).rename({\"track_id\":\"track_id_source\"}).assign(track_id = id_ib, name = name))\n",
    "                \n",
    "        # Concatenate all tracks in a new source\n",
    "        eib = xr.concat(tracks_matched, dim = \"record\")\n",
    "        #Â Remove track_id_source\n",
    "        eib = eib.drop_vars(\"track_id_source\")\n",
    "        # Filter 6-hourly\n",
    "        eib = eib.where(eib.time.dt.hour % 6 == 0, drop = True)\n",
    "    \n",
    "        # =============   RE-ADD IBTRACS ATTRIBUTES  ============= #\n",
    "        print(\"... IBTrACS Attributes ...\")\n",
    "        ## Transform into dataframes\n",
    "        ib_df = ib.to_dataframe()\n",
    "        teib_df = eib.sel(source = \"IBTrACS\").squeeze().reset_coords().to_dataframe()\n",
    "        # Merge data\n",
    "        M = teib_df[[\"time\",\"track_id\"]].reset_index().merge(ib_df, how = \"left\", on = [\"time\", \"track_id\"])\n",
    "        assert len(M) == len(teib_df) # Check that no duplicate were created\n",
    "        # Transform to xarray and select attributes\n",
    "        M_xr = M.set_index(\"record\").to_xarray()\n",
    "        # Merge back into the extended ibtracs\n",
    "        eib = eib.reset_coords().merge(M_xr.drop_vars([\"lon\", \"lat\",]))\n",
    "\n",
    "        # =============   COMPUTE TRANSLATION SPEED AND AZIMUTH  ============= #\n",
    "        print(\"... Computing Translation speed and azimuth ...\")\n",
    "        S = []\n",
    "        A = []\n",
    "        for ds in tqdm(eib.source):\n",
    "            eib_ds = eib.sel(source = ds) # Subset source\n",
    "            eib_ds = eib_ds.where(~np.isnan(eib_ds.lon), drop = True) # Remove points where given source does not have data\n",
    "            speed = huracanpy.calc.translation_speed(eib_ds.lon, eib_ds.lat, eib_ds.time, eib_ds.track_id)\n",
    "            S.append(speed)\n",
    "            azimuth = huracanpy.calc.azimuth(eib_ds.lon, eib_ds.lat, eib_ds.track_id)\n",
    "            A.append(azimuth)\n",
    "        eib[\"translation_speed\"] = xr.concat(S, dim = \"source\")\n",
    "        eib[\"azimuth\"] = xr.concat(A, dim = \"source\")\n",
    "    \n",
    "        # =============   ADD CPS PARAMETERS  ============= #\n",
    "        print(\"... Adding CPS parameters ...\")\n",
    "        with open(\"input/tracks_with_CPS_data.pkl\", \"rb\") as handle:\n",
    "            tracks_with_CPS = pkl.load(handle)\n",
    "            \n",
    "        L = []\n",
    "        for ds in tqdm(tracks_with_CPS):\n",
    "            # Transform the sources into dataframes\n",
    "            t_CPS_df = tracks_with_CPS[ds].to_dataframe()\n",
    "            teib_df = eib.sel(source = ds).squeeze().reset_coords().to_dataframe()\n",
    "            for var in [\"lon\", \"lat\"]: # Round up the coordinates for the matching to work\n",
    "                N = 0\n",
    "                t_CPS_df[var] = np.round(t_CPS_df[var], N)\n",
    "                teib_df[var] = np.round(teib_df[var], N)\n",
    "            # Merge extended ib and CPS source on lon, lat and time\n",
    "            t_CPS_df = t_CPS_df[~t_CPS_df[[\"lon\", \"lat\", \"time\",]].duplicated()] # Remove duplicates\n",
    "            M = teib_df[[\"lon\", \"lat\", \"time\",]].reset_index().merge(t_CPS_df[[\"lon\", \"lat\", \"time\", \"vtl\", \"vtu\", \"b\"]], how = \"left\", on = [\"lon\", \"lat\", \"time\"])\n",
    "            assert len(M) == len(teib_df) # Check that no duplicate were created\n",
    "            M_xr = M.set_index(\"record\").to_xarray().assign(source = ds).set_coords(\"source\")[[\"vtu\", \"vtl\", \"b\"]] # convert back to source and format\n",
    "            L.append(M_xr)\n",
    "        CPS = xr.concat(L, dim = \"source\") # Merge the CPS variables over sources\n",
    "        eib = eib.merge(CPS) # Merge back into the full extended ibtracs source\n",
    "\n",
    "        # Compute 1-day rolling means of CPS parameters\n",
    "        eib_df = eib.to_dataframe()\n",
    "        CPS_roll = eib_df.reset_index().set_index(\"record\").sort_values(\"time\").groupby([\"source\", \"track_id\"]).rolling(5, center = True)[[\"vtu\", \"vtl\", \"b\"]].mean()\n",
    "        eib[[\"vtu_roll\", \"vtl_roll\", \"b_roll\"]] = CPS_roll.reset_index().set_index([\"source\", \"record\"]).to_xarray()[[\"vtu\", \"vtl\", \"b\"]]\n",
    "\n",
    "        # =============   ADD WCSI FLAGS  ============= #\n",
    "        print(\"... Adding WCSI flags ...\")\n",
    "        with open(\"input/tracks_WCSI.pkl\", \"rb\") as handle:\n",
    "            tracks_WCSI = pkl.load(handle)\n",
    "            \n",
    "        L = []\n",
    "        for ds in tqdm(tracks_WCSI):\n",
    "            # Transform the sources into dataframes\n",
    "            t_WCSI_df = tracks_WCSI[ds].to_dataframe()\n",
    "            teib_df = eib.sel(source = ds).squeeze().reset_coords().to_dataframe()\n",
    "            for var in [\"lon\", \"lat\"]: # Round up the coordinates for the matching to work\n",
    "                N = 0\n",
    "                t_WCSI_df[var] = np.round(t_WCSI_df[var], N)\n",
    "                teib_df[var] = np.round(teib_df[var], N)\n",
    "            # Merge extended ib and WCSI source on lon, lat and time\n",
    "            t_WCSI_df = t_WCSI_df[~t_WCSI_df[[\"lon\", \"lat\", \"time\",]].duplicated()] # Remove duplicates\n",
    "            M = teib_df[[\"lon\", \"lat\", \"time\",]].reset_index().merge(t_WCSI_df[[\"lon\", \"lat\", \"time\", \"is_tc\"]], how = \"left\", on = [\"lon\", \"lat\", \"time\"])\n",
    "            assert len(M) == len(teib_df) # Check that no duplicate were created\n",
    "            M = M.rename(columns = {\"is_tc\":\"WCSI\"})\n",
    "            M_xr = M.set_index(\"record\").to_xarray().assign(source = ds).set_coords(\"source\")[[\"WCSI\"]] # convert back to source and format\n",
    "            L.append(M_xr)\n",
    "        WCSI = xr.concat(L, dim = \"source\") # Merge the WCSI variables over sources\n",
    "        eib = eib.merge(WCSI) # Merge back into the full extended ibtracs source\n",
    "        eib = eib.assign(WCSI = eib.WCSI.fillna(False).astype(bool)) # Convert to boolean\n",
    "\n",
    "        # =============   ADD IS_TC & IS_ETC FLAGS  ============= #\n",
    "        print(\"... Adding is_tc & is_etc flags ...\")\n",
    "\n",
    "        # Is TC if WCSI (TRACK with CPS) or TC (SyCLoPS) or TS nature (IBTrACS)\n",
    "        eib[\"is_tc\"] = (eib.WCSI == True) | (eib.short_label == \"TC\")\n",
    "        eib.is_tc.loc[dict(source=\"IBTrACS\")] = (eib.nature == \"TS\")\n",
    "\n",
    "        # Is ETC if Cold Core Asymetric (CCA, TRACK with CPS) or EX (SyCLoPS) or ET nature (IBTrACS)\n",
    "        eib[\"CCA\"] = (np.abs(eib.b_roll) > 15) & (eib.vtl_roll < 0)\n",
    "        eib[\"is_etc\"] = (eib.CCA) | (eib.short_label == \"EX\")\n",
    "        eib.is_etc.loc[dict(source=\"IBTrACS\")] = (eib.nature == \"ET\")\n",
    "\n",
    "        # =============   ADD ET FLAGS  ============= #\n",
    "        print(\"... Adding ET flags ...\")\n",
    "\n",
    "        # Create ET variable\n",
    "        eib[\"ET\"]  = xr.DataArray(\n",
    "            np.full(eib['lon'].shape, np.nan, dtype=object),\n",
    "            dims=eib.dims,\n",
    "            coords=eib.coords\n",
    "        )\n",
    "\n",
    "        # Convert to dataframe to be able to group by two variables\n",
    "        eib_df = eib.to_dataframe().reset_index()\n",
    "\n",
    "        # Identify last TS point\n",
    "        TS = eib_df[eib_df.is_tc]\n",
    "        last_TS = TS.sort_values(\"time\").groupby([\"source\", \"track_id\"]).last()[[\"record\", \"time\"]].reset_index()\n",
    "        eib.ET.loc[dict(record = last_TS.record.to_xarray(), source = last_TS.source.to_xarray())] = +1\n",
    "\n",
    "        # Subset points post ET onset\n",
    "        tmp = eib_df.merge(last_TS, on = [\"track_id\", \"source\"], suffixes=['', '_onset'])\n",
    "        tmp = tmp.assign(delta = tmp.time - tmp.time_onset)\n",
    "        post_onset = tmp[tmp.delta >= np.timedelta64(0)]\n",
    "\n",
    "        # Identify first ETC point\n",
    "        EX = post_onset[post_onset.is_etc]\n",
    "        first_EX = EX.sort_values(\"time\").groupby([\"source\", \"track_id\"]).first()[[\"record\", \"time\"]].reset_index()\n",
    "        eib.ET.loc[dict(record = first_EX.record.to_xarray(), source = first_EX.source.to_xarray())] = -1\n",
    "\n",
    "        # Convert ET to int8\n",
    "        eib = eib.assign(ET = eib.ET.fillna(0).astype(np.int8))\n",
    "        \n",
    "        # =============   SAVE  ============= #\n",
    "        print(\"... Saving ...\")\n",
    "        ## Specify time encoding\n",
    "        eib.time.encoding['units'] = 'seconds since 1900-01-01'\n",
    "        eib.time.encoding['calendar'] = 'standard'\n",
    "        ## Changes types for lighter files\n",
    "        ### Convert to float32\n",
    "        varlist = list(eib.variables.keys())\n",
    "        floatvars = list(set(varlist) & (set([\"b_roll\", \"vtu_roll\", \"vtl_roll\", \"azimuth\", \"translation_speed\", \"lon\", \"lat\"])))\n",
    "        for var in floatvars:\n",
    "            eib[var] = eib[var].astype(np.float32)\n",
    "        ## Actual saving\n",
    "        eib.to_netcdf(\"extended-ibtracs/extended-ibtracs_\"+b+\".nc\")\n",
    "        \n",
    "        print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huracanpy",
   "language": "python",
   "name": "huracanpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
